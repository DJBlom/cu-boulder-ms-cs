{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11848,"databundleVersionId":862157,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# Note: Ignore the file walk as there are thousands of files.\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-23T23:17:07.550914Z","iopub.execute_input":"2025-05-23T23:17:07.551204Z","iopub.status.idle":"2025-05-23T23:17:09.521810Z","shell.execute_reply.started":"2025-05-23T23:17:07.551178Z","shell.execute_reply":"2025-05-23T23:17:09.520859Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Project Overview: CNN-Driven Histopathologic Cancer Detection\n\nIn this notebook, I examine the PatchCamelyon (PCam) dataset—originally introduced by Cukierski et al. (2018)—to develop a convolutional neural network capable of detecting metastatic tissue within lymph node histology images. By working with a de-duplicated subset of PCam, we streamline preprocessing and focus on accurately predicting whether the central 32×32-pixel region of each 96×96 patch contains any tumor pixels.","metadata":{}},{"cell_type":"markdown","source":"First, we’ll carry out data preprocessing to gain a clearer understanding of the dataset’s characteristics.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Preliminary Data Exploration and Assessment","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('/kaggle/input/histopathologic-cancer-detection/train_labels.csv')\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T23:17:16.135125Z","iopub.execute_input":"2025-05-23T23:17:16.135434Z","iopub.status.idle":"2025-05-23T23:17:16.524604Z","shell.execute_reply.started":"2025-05-23T23:17:16.135410Z","shell.execute_reply":"2025-05-23T23:17:16.523515Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                         id  label\n0  f38a6374c348f90b587e046aac6079959adf3835      0\n1  c18f2d887b7ae4f6742ee445113fa1aef383ed77      1\n2  755db6279dae599ebb4d39a9123cce439965282d      0\n3  bc3f0c64fb968ff4a8bd33af6971ecae77c75e08      0\n4  068aba587a4950175d04c680d38943fd488d6a9d      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>f38a6374c348f90b587e046aac6079959adf3835</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>c18f2d887b7ae4f6742ee445113fa1aef383ed77</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>755db6279dae599ebb4d39a9123cce439965282d</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>bc3f0c64fb968ff4a8bd33af6971ecae77c75e08</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>068aba587a4950175d04c680d38943fd488d6a9d</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"Once the data is loaded, it’s crucial to examine its class distribution to understand how samples are balanced.","metadata":{}},{"cell_type":"code","source":"print(df['label'].value_counts())\nprint(df.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T23:17:22.401366Z","iopub.execute_input":"2025-05-23T23:17:22.401657Z","iopub.status.idle":"2025-05-23T23:17:22.430000Z","shell.execute_reply.started":"2025-05-23T23:17:22.401634Z","shell.execute_reply":"2025-05-23T23:17:22.429077Z"}},"outputs":[{"name":"stdout","text":"label\n0    130908\n1     89117\nName: count, dtype: int64\nid       0\nlabel    0\ndtype: int64\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"To get a better feel for the data, I recommend three quick visualizations you can drop into your notebook (using only Matplotlib and your existing generators):","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nlabel_counts = df['label'].value_counts()\ntotal = len(df)\n\n# Plot\nplt.figure(figsize=(6,4))\nax = sns.countplot(data=df, x='label')\n\nfor p in ax.patches:\n    count = p.get_height()\n    percent = f'{100 * count / total:.2f}%'\n    x = p.get_x() + p.get_width() / 2\n    y = p.get_height()\n    ax.text(x, y + total * 0.01, percent, ha='center', va='bottom', fontsize=12)\n\n# Format\nplt.title(\"Label Distribution\")\nplt.xlabel(\"Label\")\nplt.ylabel(\"Count\")\nplt.ylim(0, label_counts.max() * 1.1)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T23:17:26.070062Z","iopub.execute_input":"2025-05-23T23:17:26.070403Z","iopub.status.idle":"2025-05-23T23:17:27.257859Z","shell.execute_reply.started":"2025-05-23T23:17:26.070378Z","shell.execute_reply":"2025-05-23T23:17:27.257020Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 600x400 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"The dataset’s class balance shows roughly 59.5 % non-cancerous patches and 40.5 % cancerous ones.","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport os\n\ndef load_image(image_id, base_path='/kaggle/input/histopathologic-cancer-detection/train'):\n    path = os.path.join(base_path, f\"{image_id}.tif\")\n    return cv2.imread(path)\n\ndef show_samples(df, label, n=5):\n    samples = df[df['label'] == label].sample(n)\n    fig, axes = plt.subplots(1, n, figsize=(15, 5))\n    for img_id, ax in zip(samples['id'], axes):\n        img = load_image(img_id)\n        ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n        ax.axis('off')\n    plt.suptitle(f\"Label: {label}\")\n    plt.show()\n\nshow_samples(df, label=0)\nshow_samples(df, label=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T01:17:16.146936Z","iopub.execute_input":"2025-05-18T01:17:16.14733Z","iopub.status.idle":"2025-05-18T01:17:16.754634Z","shell.execute_reply.started":"2025-05-18T01:17:16.147304Z","shell.execute_reply":"2025-05-18T01:17:16.75383Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"All patches share identical dimensions. In my view, inspecting an image array’s shape attribute is the quickest way to confirm its width, height, and three RGB channels.","metadata":{}},{"cell_type":"code","source":"img = load_image(df['id'][0])\nprint(\"Image shape:\", img.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T01:17:16.75555Z","iopub.execute_input":"2025-05-18T01:17:16.755822Z","iopub.status.idle":"2025-05-18T01:17:16.761332Z","shell.execute_reply.started":"2025-05-18T01:17:16.755804Z","shell.execute_reply":"2025-05-18T01:17:16.760522Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To explore channel-wise intensity patterns, I’d recommend comparing the red/green/blue histograms of a cancerous patch versus a non-cancerous one. Here’s a quick example you can drop into your notebook (adjusting paths as needed):","metadata":{}},{"cell_type":"code","source":"def plot_color_distribution_histogram(image):\n    colors = ['r', 'g', 'b']\n    for i, color in enumerate(colors):\n        plt.hist(image[..., i].ravel(), bins=256, color=color, alpha=0.5)\n    plt.title(\"Pixel Intensity Distribution\")\n    plt.xlabel(\"Pixel value\")\n    plt.ylabel(\"Frequency\")\n    plt.show()\n\nimg_tumor = load_image(df[df['label'] == 1].sample(1).iloc[0]['id'])\nimg_normal = load_image(df[df['label'] == 0].sample(1).iloc[0]['id'])\n\nplot_color_distribution_histogram(img_tumor)\nplot_color_distribution_histogram(img_normal)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T01:17:16.762221Z","iopub.execute_input":"2025-05-18T01:17:16.762524Z","iopub.status.idle":"2025-05-18T01:17:18.607157Z","shell.execute_reply.started":"2025-05-18T01:17:16.762505Z","shell.execute_reply":"2025-05-18T01:17:18.606504Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The visualization suggests that cancerous regions tend to appear as areas of more intense, concentrated coloration, while non-cancerous samples exhibit a broader, more diffuse color spread. Of course, this insight is drawn from only one example, so I’d be careful not to overinterpret it. Since our preliminary checks found no duplicates or missing values, no additional data cleaning is required at this point.","metadata":{}},{"cell_type":"code","source":"def tissue_ratio(img, threshold=200):\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    return np.mean(gray < threshold)\n\n# Use only a sample for EDA\nsample_ids = df['id'].sample(2000, random_state=42)\n\nratios = sample_ids.apply(lambda id_: tissue_ratio(load_image(id_)))\ndf['tissue_ratio'] = ratios\n\nsns.histplot(data=df, x='tissue_ratio', hue='label', bins=30, kde=True)\nplt.title(\"Tissue Area Ratio Distribution\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T01:17:18.607904Z","iopub.execute_input":"2025-05-18T01:17:18.608194Z","iopub.status.idle":"2025-05-18T01:17:22.896851Z","shell.execute_reply.started":"2025-05-18T01:17:18.608167Z","shell.execute_reply":"2025-05-18T01:17:22.896233Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Sample Analysis\n\nIn a randomly selected subset of 2 000 images, cancerous patches exhibit a noticeably higher tissue density and more concentrated regions of interest compared to non-cancerous samples, which appear more diffusely distributed. This observation aligns with our earlier pixel-intensity histogram, reinforcing the notion that malignant regions tend to present distinct visual signatures.","metadata":{}},{"cell_type":"markdown","source":"#### Image Import Preparation\n\nBefore feeding the data into our Keras generators, we augment the DataFrame with a filename column (appending the “.tif” extension) and cast the label field to string format—both prerequisites for seamless ingestion by ImageDataGenerator.","metadata":{}},{"cell_type":"code","source":"df['label_str'] = df['label'].astype(str)\ndf['filename'] = df['id'] + '.tif'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T05:27:05.935258Z","iopub.execute_input":"2025-05-20T05:27:05.935547Z","iopub.status.idle":"2025-05-20T05:27:06.020054Z","shell.execute_reply.started":"2025-05-20T05:27:05.935525Z","shell.execute_reply":"2025-05-20T05:27:06.019504Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Dataset Partitioning\n\nTo prepare for model training, we need to split our data into training and validation sets. Since loading the entire corpus can overwhelm Kaggle’s memory, I first draw a random sample of 40 000 images. In my view, this subset size offers a good trade-off—fast iteration without sacrificing representativeness—before applying the final train/test split.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndf_sampled, _ = train_test_split(\n    df,\n    train_size=40000,\n    stratify=df['label'],\n    random_state=42\n)\n\ntrain_df, val_df = train_test_split(\n    df_sampled,\n    test_size=0.2,                # 20% for validation\n    stratify=df_sampled['label'],         # maintain class balance\n    random_state=42               # reproducibility\n)\n\nprint(\"Train label distribution:\")\nprint(train_df['label'].value_counts(normalize=True))\n\nprint(\"\\nValidation label distribution:\")\nprint(val_df['label'].value_counts(normalize=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T05:27:08.822866Z","iopub.execute_input":"2025-05-20T05:27:08.823229Z","iopub.status.idle":"2025-05-20T05:27:08.9813Z","shell.execute_reply.started":"2025-05-20T05:27:08.823207Z","shell.execute_reply":"2025-05-20T05:27:08.980471Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We’ll construct a TensorFlow Dataset that ingests the full image set and immediately rescales pixel values from the 0–255 range to 0–1 by dividing by 255. By applying .cache() and .prefetch(), we ensure that data loading and preprocessing are overlapped with GPU execution, maximizing throughput and minimizing I/O stalls.","metadata":{}},{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport tensorflow as tf\n\nBATCH_SIZE = 32\n\ndef load_image_cv2(path):\n    img = cv2.imread(path)\n    img = cv2.resize(img, (96, 96))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return img.astype(np.float32) / 255.0\n\ndef data_generator(paths, labels):\n    for path, label in zip(paths, labels):\n        img = load_image_cv2(path)\n        yield img, label\n\ntrain_paths = [f\"/kaggle/input/histopathologic-cancer-detection/train/{f}\" for f in train_df['filename']]\ntrain_labels = train_df['label'].values.astype(np.float32)\n\nval_paths = [f\"/kaggle/input/histopathologic-cancer-detection/train/{f}\" for f in val_df['filename']]\nval_labels = val_df['label'].values.astype(np.float32)\n\ntrain_data = tf.data.Dataset.from_generator(\n    lambda: data_generator(train_paths, train_labels),\n    output_signature=(\n        tf.TensorSpec(shape=(96, 96, 3), dtype=tf.float32),\n        tf.TensorSpec(shape=(), dtype=tf.float32)\n    )\n).shuffle(1024).repeat().batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n\nval_data = tf.data.Dataset.from_generator(\n    lambda: data_generator(val_paths, val_labels),\n    output_signature=(\n        tf.TensorSpec(shape=(96, 96, 3), dtype=tf.float32),\n        tf.TensorSpec(shape=(), dtype=tf.float32)\n    )\n).batch(BATCH_SIZE, drop_remainder=True).cache().prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T05:27:13.42103Z","iopub.execute_input":"2025-05-20T05:27:13.421746Z","iopub.status.idle":"2025-05-20T05:27:13.526708Z","shell.execute_reply.started":"2025-05-20T05:27:13.42172Z","shell.execute_reply":"2025-05-20T05:27:13.525993Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The following snippet confirms that the dataset has been loaded correctly—previously, I encountered cases where all labels were erroneously set to either 0 or 1.","metadata":{}},{"cell_type":"code","source":"for images, labels in val_data.take(1):\n    print(np.unique(labels.numpy(), return_counts=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T03:30:09.803413Z","iopub.execute_input":"2025-05-20T03:30:09.803955Z","iopub.status.idle":"2025-05-20T03:30:10.603175Z","shell.execute_reply.started":"2025-05-20T03:30:09.803932Z","shell.execute_reply":"2025-05-20T03:30:10.602412Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Architecture","metadata":{}},{"cell_type":"markdown","source":"We’ll evaluate two distinct CNN designs for this task:\n\n- A lightweight, batch‐normalized model with a handful of convolutional layers for efficient training and reduced overfitting.\n\n- A deeper, VGGNet‐inspired architecture as covered in our coursework, to compare its feature-extraction capabilities against the simpler network.","metadata":{}},{"cell_type":"markdown","source":"# Streamlined CNN with Interleaved Batch Normalization\n\nThis configuration implements a concise convolutional network that mirrors the VGG-style block structure—but with fewer layers for simplicity. After each convolutional layer, we apply Batch Normalization to stabilize activations and accelerate convergence. By combining a leaner depth with normalization, this architecture is designed to train efficiently while mitigating overfitting.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models, callbacks\n\ndef build_basic(input_shape=(96, 96, 3)):\n    model = models.Sequential(name=\"Basic_Model\")\n    model.add(layers.Input(shape=input_shape))\n\n    model.add(layers.Conv2D(32, (3,3), activation='relu', padding='same'))\n    model.add(layers.BatchNormalization())\n\n    model.add(layers.Conv2D(64, (3,3), activation='relu', padding='same'))\n    model.add(layers.BatchNormalization())\n\n    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n    model.add(layers.BatchNormalization())\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1, activation='sigmoid'))\n    return model\n\nbasic_model = build_basic()\nbasic_model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T04:55:00.943485Z","iopub.execute_input":"2025-05-19T04:55:00.943791Z","iopub.status.idle":"2025-05-19T04:55:02.209029Z","shell.execute_reply.started":"2025-05-19T04:55:00.943768Z","shell.execute_reply":"2025-05-19T04:55:02.208405Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Alternative Architecture: VGGNet Backbone\n\nOur implementation builds on the classic VGGNet pattern: sequential blocks of convolutional layers followed by max‐pooling, repeated three times for clarity. Within each block, we apply ReLU activations to introduce nonlinearity, then use a sigmoid activation at the output to produce a binary prediction. To guard against overfitting, we insert dropout before the final classification layer—helping the network generalize more reliably.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\n\ndef build_vgg_like(input_shape=(96, 96, 3)):\n    model = models.Sequential(name=\"VGGNet\")\n    model.add(layers.Input(shape=input_shape))\n    \n    model.add(layers.Conv2D(32, (3,3), activation='relu', padding='same'))\n    model.add(layers.Conv2D(32, (3,3), activation='relu', padding='same'))\n    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n    \n    model.add(layers.Conv2D(64, (3,3), activation='relu', padding='same'))\n    model.add(layers.Conv2D(64, (3,3), activation='relu', padding='same'))\n    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n    \n    model.add(layers.Conv2D(128, (3,3), activation='relu', padding='same'))\n    model.add(layers.Conv2D(128, (3,3), activation='relu', padding='same'))\n    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(256, activation='relu'))\n    model.add(layers.Dense(256, activation='relu'))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(1, activation='sigmoid'))\n    \n    return model\n\nvgg_model = build_vgg_like()\nvgg_model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T03:30:20.535337Z","iopub.execute_input":"2025-05-20T03:30:20.53561Z","iopub.status.idle":"2025-05-20T03:30:21.894595Z","shell.execute_reply.started":"2025-05-20T03:30:20.535588Z","shell.execute_reply":"2025-05-20T03:30:21.894044Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Results and Analysis\n\nAt this stage, we’ll begin by training our baseline CNN and rigorously evaluating its performance using the confusion matrix, ROC AUC, and—crucially—the false-negative rate given the clinical implications. We’ll then apply the same evaluation pipeline to our VGGNet-based model. Finally, we’ll identify the stronger of the two and conduct targeted hyperparameter tuning—adjusting optimizers, loss functions, and other settings—to maximize its reliability. In every training run, we’ll calculate and explicitly specify the steps-per-epoch to ensure each model consumes the entire dataset without running dry.","metadata":{}},{"cell_type":"code","source":"steps_per_epoch = len(train_df) // BATCH_SIZE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T03:30:27.518173Z","iopub.execute_input":"2025-05-20T03:30:27.518446Z","iopub.status.idle":"2025-05-20T03:30:27.522032Z","shell.execute_reply.started":"2025-05-20T03:30:27.518424Z","shell.execute_reply":"2025-05-20T03:30:27.521408Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## InitialModel","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\n\nbasic_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\ncallbacks_list = [\n    # This helps stop the model early\n    callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=3,\n        restore_best_weights=True\n    ),\n    # Reduces learning rate when plateued\n    callbacks.ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.2,\n        patience=3,\n        min_lr=1e-6,\n        verbose=1\n    ),\n    # save best model\n    callbacks.ModelCheckpoint(\n        filepath='best_basic_model.keras',\n        monitor='val_accuracy',\n        save_best_only=True,\n        verbose=1\n    )\n]\n\nbm_history = basic_model.fit(\n    train_data, \n    validation_data = val_data, \n    epochs=10, \n    steps_per_epoch=steps_per_epoch,\n    callbacks=callbacks_list\n)\nbasic_model.evaluate(val_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T05:23:15.110513Z","iopub.execute_input":"2025-05-19T05:23:15.110918Z","iopub.status.idle":"2025-05-19T05:27:32.393807Z","shell.execute_reply.started":"2025-05-19T05:23:15.110892Z","shell.execute_reply":"2025-05-19T05:27:32.393001Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Having trained our inaugural model, let’s visualize its performance by plotting the accuracy and loss curves over each epoch.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Accuracy plot\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(bm_history.history['accuracy'], label='Train Accuracy')\nplt.plot(bm_history.history['val_accuracy'], label='Val Accuracy')\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\n# Loss plot\nplt.subplot(1, 2, 2)\nplt.plot(bm_history.history['loss'], label='Train Loss')\nplt.plot(bm_history.history['val_loss'], label='Val Loss')\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T05:27:39.177476Z","iopub.execute_input":"2025-05-19T05:27:39.178077Z","iopub.status.idle":"2025-05-19T05:27:39.537615Z","shell.execute_reply.started":"2025-05-19T05:27:39.178051Z","shell.execute_reply":"2025-05-19T05:27:39.536683Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Reviewing the training curves, validation accuracy rises at first but then declines, while validation loss spikes sharply as accuracy falls—classic signs of overfitting and unstable generalization. The next utility function will pair model predictions with true labels, setting us up to calculate the confusion matrix and ROC AUC.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef get_y_true_and_y_pred(val_data, model, threshold):\n\n    # Predict on validation set\n    y_true = []\n    y_pred_probs = []\n    \n    for batch_x, batch_y in val_data:\n        preds = model.predict(batch_x, verbose=0)\n        y_pred_probs.extend(preds.ravel())  # Flatten to 1D list\n        y_true.extend(batch_y.numpy().ravel())  # Convert from tensor to NumPy array\n    \n    # Convert to NumPy arrays\n    y_true = np.array(y_true)\n    y_pred_probs = np.array(y_pred_probs)\n    \n    # Convert probabilities to binary predictions\n    y_pred = (y_pred_probs > threshold).astype(int)\n    return y_true, y_pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T05:25:53.682415Z","iopub.execute_input":"2025-05-20T05:25:53.68275Z","iopub.status.idle":"2025-05-20T05:25:53.688962Z","shell.execute_reply.started":"2025-05-20T05:25:53.682726Z","shell.execute_reply":"2025-05-20T05:25:53.687925Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_true_bm, y_pred_bm = get_y_true_and_y_pred(val_data, basic_model, 0.5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T05:27:49.432175Z","iopub.execute_input":"2025-05-19T05:27:49.432824Z","iopub.status.idle":"2025-05-19T05:28:04.036025Z","shell.execute_reply.started":"2025-05-19T05:27:49.432799Z","shell.execute_reply":"2025-05-19T05:28:04.035089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Confusion Matrix\ncm = confusion_matrix(y_true_bm, y_pred_bm)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot(cmap='Blues')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T05:28:07.41476Z","iopub.execute_input":"2025-05-19T05:28:07.415478Z","iopub.status.idle":"2025-05-19T05:28:07.640064Z","shell.execute_reply.started":"2025-05-19T05:28:07.415451Z","shell.execute_reply":"2025-05-19T05:28:07.639251Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Our confusion matrix still reveals a substantial number of false negatives, which poses an unacceptable risk in any clinical application.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\nfpr, tpr, _ = roc_curve(y_true_bm, y_pred_probs_bm)\nroc_auc = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate (Recall)')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc='lower right')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T05:28:15.300051Z","iopub.execute_input":"2025-05-19T05:28:15.300395Z","iopub.status.idle":"2025-05-19T05:28:15.490467Z","shell.execute_reply.started":"2025-05-19T05:28:15.300369Z","shell.execute_reply":"2025-05-19T05:28:15.489517Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The ROC curve reveals an AUC of 0.84—demonstrating respectable discrimination, though there remains considerable scope for improvement.","metadata":{}},{"cell_type":"markdown","source":"## Architectural Overview of the VGGNet-Based Model","metadata":{}},{"cell_type":"code","source":"train_data = tf.data.Dataset.from_generator(\n    lambda: data_generator(train_paths, train_labels),\n    output_signature=(\n        tf.TensorSpec(shape=(96, 96, 3), dtype=tf.float32),\n        tf.TensorSpec(shape=(), dtype=tf.float32)\n    )\n).shuffle(1024).repeat().batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n\nval_data = tf.data.Dataset.from_generator(\n    lambda: data_generator(val_paths, val_labels),\n    output_signature=(\n        tf.TensorSpec(shape=(96, 96, 3), dtype=tf.float32),\n        tf.TensorSpec(shape=(), dtype=tf.float32)\n    )\n).batch(BATCH_SIZE, drop_remainder=True).cache().prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T03:33:21.625136Z","iopub.execute_input":"2025-05-20T03:33:21.625423Z","iopub.status.idle":"2025-05-20T03:33:21.688522Z","shell.execute_reply.started":"2025-05-20T03:33:21.625401Z","shell.execute_reply":"2025-05-20T03:33:21.687762Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models, callbacks\n\nvgg_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\ncallbacks_list = [\n    callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=3,\n        restore_best_weights=True\n    ),\n    callbacks.ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.2,\n        patience=3,\n        min_lr=1e-6\n    ),\n    # save best model\n    callbacks.ModelCheckpoint(\n        filepath='best_vgg_model.keras',\n        monitor='val_accuracy',\n        save_best_only=True,\n        verbose=1\n    )\n]\n\nvgg_history = vgg_model.fit(\n    train_data, \n    validation_data = val_data, \n    epochs=10,\n    steps_per_epoch=steps_per_epoch,\n    callbacks=callbacks_list\n)\nvgg_model.evaluate(val_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T03:33:25.063627Z","iopub.execute_input":"2025-05-20T03:33:25.063918Z","iopub.status.idle":"2025-05-20T03:47:42.404572Z","shell.execute_reply.started":"2025-05-20T03:33:25.063895Z","shell.execute_reply":"2025-05-20T03:47:42.40401Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Accuracy plot\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(vgg_history.history['accuracy'], label='Train Accuracy')\nplt.plot(vgg_history.history['val_accuracy'], label='Val Accuracy')\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\n# Loss plot\nplt.subplot(1, 2, 2)\nplt.plot(vgg_history.history['loss'], label='Train Loss')\nplt.plot(vgg_history.history['val_loss'], label='Val Loss')\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T03:48:11.164822Z","iopub.execute_input":"2025-05-20T03:48:11.165121Z","iopub.status.idle":"2025-05-20T03:48:11.574093Z","shell.execute_reply.started":"2025-05-20T03:48:11.165099Z","shell.execute_reply":"2025-05-20T03:48:11.573233Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We observe that training loss continues to decrease while validation loss climbs—an unmistakable sign of overfitting. Moreover, the stagnation and volatility of validation loss imply that our learning rate may still be too high, causing the optimizer to overshoot the true minima.","metadata":{}},{"cell_type":"code","source":"y_true_vgg, y_pred_vgg = get_y_true_and_y_pred(val_data, vgg_model, 0.5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T03:48:33.674578Z","iopub.execute_input":"2025-05-20T03:48:33.675322Z","iopub.status.idle":"2025-05-20T03:48:47.965417Z","shell.execute_reply.started":"2025-05-20T03:48:33.675295Z","shell.execute_reply":"2025-05-20T03:48:47.964773Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Confusion Matrix\ncm = confusion_matrix(y_true_vgg, y_pred_vgg)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot(cmap='Blues')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T03:48:50.151588Z","iopub.execute_input":"2025-05-20T03:48:50.151862Z","iopub.status.idle":"2025-05-20T03:48:50.32193Z","shell.execute_reply.started":"2025-05-20T03:48:50.15184Z","shell.execute_reply":"2025-05-20T03:48:50.321295Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\nfpr, tpr, _ = roc_curve(y_true_vgg, y_pred_probs_vgg)\nroc_auc = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate (Recall)')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc='lower right')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T03:48:53.649225Z","iopub.execute_input":"2025-05-20T03:48:53.649473Z","iopub.status.idle":"2025-05-20T03:48:53.81664Z","shell.execute_reply.started":"2025-05-20T03:48:53.649456Z","shell.execute_reply":"2025-05-20T03:48:53.815897Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Given the clinical importance of minimizing false negatives, our next step is to refine the model’s hyperparameters. We’ll begin by lowering the learning rate from its default of 1×10⁻³ to 1×10⁻⁴ to encourage more gradual, stable convergence.","metadata":{}},{"cell_type":"markdown","source":"## Fine-Tuning VGGNet Through Learning Rate Optimization","metadata":{}},{"cell_type":"code","source":"train_data = tf.data.Dataset.from_generator(\n    lambda: data_generator(train_paths, train_labels),\n    output_signature=(\n        tf.TensorSpec(shape=(96, 96, 3), dtype=tf.float32),\n        tf.TensorSpec(shape=(), dtype=tf.float32)\n    )\n).shuffle(1024).repeat().batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n\nval_data = tf.data.Dataset.from_generator(\n    lambda: data_generator(val_paths, val_labels),\n    output_signature=(\n        tf.TensorSpec(shape=(96, 96, 3), dtype=tf.float32),\n        tf.TensorSpec(shape=(), dtype=tf.float32)\n    )\n).batch(32, drop_remainder=True).cache().prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T04:05:52.467249Z","iopub.execute_input":"2025-05-20T04:05:52.467528Z","iopub.status.idle":"2025-05-20T04:05:52.536672Z","shell.execute_reply.started":"2025-05-20T04:05:52.467506Z","shell.execute_reply":"2025-05-20T04:05:52.535903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models, callbacks\n\nnew_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n\nvgg_model.compile(optimizer=new_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n\ncallbacks_list = [\n    callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=3,\n        restore_best_weights=True\n    ),\n    callbacks.ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.2,\n        patience=3,\n        min_lr=1e-6\n    ),\n    # save best model\n    callbacks.ModelCheckpoint(\n        filepath='best_vgg_model_v1.keras',\n        monitor='val_accuracy',\n        save_best_only=True,\n        verbose=1\n    )\n]\n\nvgg_history_v2 = vgg_model.fit(\n    train_data, \n    validation_data = val_data, \n    epochs=10,\n    steps_per_epoch=steps_per_epoch,\n    callbacks=callbacks_list\n)\nvgg_model.evaluate(val_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T04:05:56.115952Z","iopub.execute_input":"2025-05-20T04:05:56.116633Z","iopub.status.idle":"2025-05-20T04:10:24.269354Z","shell.execute_reply.started":"2025-05-20T04:05:56.11661Z","shell.execute_reply":"2025-05-20T04:10:24.268771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Accuracy plot\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(vgg_history_v2.history['accuracy'], label='Train Accuracy')\nplt.plot(vgg_history_v2.history['val_accuracy'], label='Val Accuracy')\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\n# Loss plot\nplt.subplot(1, 2, 2)\nplt.plot(vgg_history_v2.history['loss'], label='Train Loss')\nplt.plot(vgg_history_v2.history['val_loss'], label='Val Loss')\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T04:27:44.981722Z","iopub.execute_input":"2025-05-20T04:27:44.982494Z","iopub.status.idle":"2025-05-20T04:27:45.30247Z","shell.execute_reply.started":"2025-05-20T04:27:44.982467Z","shell.execute_reply":"2025-05-20T04:27:45.301727Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Our training curves show a widening divergence—training loss keeps falling even as validation loss climbs—indicating overfitting. To mitigate this, we’ll introduce an additional dropout layer into the architecture.","metadata":{}},{"cell_type":"markdown","source":"## Augmenting VGGNet with an Extra Dropout Mechanism","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\n\ndef build_vgg_like_v2(input_shape=(96, 96, 3)):\n    model = models.Sequential(name=\"VGGNetv2\")\n    model.add(layers.Input(shape=input_shape))\n    \n    model.add(layers.Conv2D(32, (3,3), activation='relu', padding='same'))\n    model.add(layers.Conv2D(32, (3,3), activation='relu', padding='same'))\n    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n    \n    model.add(layers.Conv2D(64, (3,3), activation='relu', padding='same'))\n    model.add(layers.Conv2D(64, (3,3), activation='relu', padding='same'))\n    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n    \n    model.add(layers.Conv2D(128, (3,3), activation='relu', padding='same'))\n    model.add(layers.Conv2D(128, (3,3), activation='relu', padding='same'))\n    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(256, activation='relu'))\n    model.add(layers.Dropout(0.3))\n    model.add(layers.Dense(256, activation='relu'))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(1, activation='sigmoid'))\n    \n    return model\n\nvgg_model_v2 = build_vgg_like_v2()\nvgg_model_v2.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T04:04:36.687108Z","iopub.execute_input":"2025-05-20T04:04:36.687401Z","iopub.status.idle":"2025-05-20T04:04:36.831471Z","shell.execute_reply.started":"2025-05-20T04:04:36.687377Z","shell.execute_reply":"2025-05-20T04:04:36.830914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = tf.data.Dataset.from_generator(\n    lambda: data_generator(train_paths, train_labels),\n    output_signature=(\n        tf.TensorSpec(shape=(96, 96, 3), dtype=tf.float32),\n        tf.TensorSpec(shape=(), dtype=tf.float32)\n    )\n).shuffle(1024).repeat().batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n\nval_data = tf.data.Dataset.from_generator(\n    lambda: data_generator(val_paths, val_labels),\n    output_signature=(\n        tf.TensorSpec(shape=(96, 96, 3), dtype=tf.float32),\n        tf.TensorSpec(shape=(), dtype=tf.float32)\n    )\n).batch(BATCH_SIZE, drop_remainder=True).cache().prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T04:04:43.026166Z","iopub.execute_input":"2025-05-20T04:04:43.026642Z","iopub.status.idle":"2025-05-20T04:04:43.095616Z","shell.execute_reply.started":"2025-05-20T04:04:43.02662Z","shell.execute_reply":"2025-05-20T04:04:43.095041Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n\nvgg_model_v2.compile(optimizer=new_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n\ncallbacks_list = [\n    callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=3,\n        restore_best_weights=True\n    ),\n    callbacks.ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.2,\n        patience=3,\n        min_lr=1e-6\n    ),\n    # save best model\n    callbacks.ModelCheckpoint(\n        filepath='best_vgg_model_v2.keras',\n        monitor='val_accuracy',\n        save_best_only=True,\n        verbose=1\n    )\n]\n\nvgg_history_v3 = vgg_model_v2.fit(\n    train_data, \n    validation_data = val_data, \n    epochs=10,\n    steps_per_epoch=steps_per_epoch,\n    callbacks=callbacks_list\n)\nvgg_model_v2.evaluate(val_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:18:25.988322Z","iopub.execute_input":"2025-05-19T06:18:25.989005Z","iopub.status.idle":"2025-05-19T06:27:08.034057Z","shell.execute_reply.started":"2025-05-19T06:18:25.988978Z","shell.execute_reply":"2025-05-19T06:27:08.033447Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Accuracy plot\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(vgg_history_v3.history['accuracy'], label='Train Accuracy')\nplt.plot(vgg_history_v3.history['val_accuracy'], label='Val Accuracy')\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\n# Loss plot\nplt.subplot(1, 2, 2)\nplt.plot(vgg_history_v3.history['loss'], label='Train Loss')\nplt.plot(vgg_history_v3.history['val_loss'], label='Val Loss')\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T06:27:30.497843Z","iopub.execute_input":"2025-05-19T06:27:30.498397Z","iopub.status.idle":"2025-05-19T06:27:30.833928Z","shell.execute_reply.started":"2025-05-19T06:27:30.498369Z","shell.execute_reply":"2025-05-19T06:27:30.833139Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We’re making clear progress: both training and validation accuracy are climbing while loss steadily decreases. For our next refinement, I’ll introduce L2 regularization into the final dense layers.","metadata":{}},{"cell_type":"markdown","source":"## Incorporating L2 Weight Decay into VGGNet Fine-Tuning","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\n\ndef build_vgg_like_v3(input_shape=(96, 96, 3)):\n    model = models.Sequential(name=\"VGGNetv3\")\n    model.add(layers.Input(shape=input_shape))\n    \n    model.add(layers.Conv2D(32, (3,3), activation='relu', padding='same'))\n    model.add(layers.Conv2D(32, (3,3), activation='relu', padding='same'))\n    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n    \n    model.add(layers.Conv2D(64, (3,3), activation='relu', padding='same'))\n    model.add(layers.Conv2D(64, (3,3), activation='relu', padding='same'))\n    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n    \n    model.add(layers.Conv2D(128, (3,3), activation='relu', padding='same'))\n    model.add(layers.Conv2D(128, (3,3), activation='relu', padding='same'))\n    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n    model.add(layers.Dropout(0.3))\n    model.add(layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(1, activation='sigmoid'))\n    \n    return model\n\nvgg_model_v3 = build_vgg_like_v3()\nvgg_model_v3.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:57:18.67115Z","iopub.execute_input":"2025-05-19T08:57:18.671414Z","iopub.status.idle":"2025-05-19T08:57:20.060396Z","shell.execute_reply.started":"2025-05-19T08:57:18.671393Z","shell.execute_reply":"2025-05-19T08:57:20.059788Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = tf.data.Dataset.from_generator(\n    lambda: data_generator(train_paths, train_labels),\n    output_signature=(\n        tf.TensorSpec(shape=(96, 96, 3), dtype=tf.float32),\n        tf.TensorSpec(shape=(), dtype=tf.float32)\n    )\n).shuffle(1024).repeat().batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n\nval_data = tf.data.Dataset.from_generator(\n    lambda: data_generator(val_paths, val_labels),\n    output_signature=(\n        tf.TensorSpec(shape=(96, 96, 3), dtype=tf.float32),\n        tf.TensorSpec(shape=(), dtype=tf.float32)\n    )\n).batch(BATCH_SIZE, drop_remainder=True).cache().prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:32:33.753218Z","iopub.execute_input":"2025-05-19T09:32:33.75348Z","iopub.status.idle":"2025-05-19T09:32:33.820724Z","shell.execute_reply.started":"2025-05-19T09:32:33.75346Z","shell.execute_reply":"2025-05-19T09:32:33.820199Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models, callbacks\n\nnew_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n\nvgg_model_v3.compile(optimizer=new_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n\ncallbacks_list = [\n    callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=3,\n        restore_best_weights=True\n    ),\n    callbacks.ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.2,\n        patience=3,\n        min_lr=1e-6\n    ),\n    # save best model\n    callbacks.ModelCheckpoint(\n        filepath='best_vgg_model_v3.keras',\n        monitor='val_accuracy',\n        save_best_only=True,\n        verbose=1\n    )\n]\n\nvgg_history_v4 = vgg_model_v3.fit(\n    train_data, \n    validation_data = val_data, \n    epochs=10,\n    steps_per_epoch=steps_per_epoch,\n    callbacks=callbacks_list\n)\nvgg_model_v3.evaluate(val_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:32:59.283476Z","iopub.execute_input":"2025-05-19T09:32:59.283747Z","iopub.status.idle":"2025-05-19T09:42:17.993461Z","shell.execute_reply.started":"2025-05-19T09:32:59.283726Z","shell.execute_reply":"2025-05-19T09:42:17.992837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Accuracy plot\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(vgg_history_v4.history['accuracy'], label='Train Accuracy')\nplt.plot(vgg_history_v4.history['val_accuracy'], label='Val Accuracy')\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\n# Loss plot\nplt.subplot(1, 2, 2)\nplt.plot(vgg_history_v4.history['loss'], label='Train Loss')\nplt.plot(vgg_history_v4.history['val_loss'], label='Val Loss')\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:42:21.918656Z","iopub.execute_input":"2025-05-19T09:42:21.919207Z","iopub.status.idle":"2025-05-19T09:42:22.235017Z","shell.execute_reply.started":"2025-05-19T09:42:21.919183Z","shell.execute_reply":"2025-05-19T09:42:22.23404Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In the training curves, validation accuracy rises in tandem with training accuracy, and validation loss closely mirrors training loss—evidence that our final hyperparameter adjustments harmonized learning effectively. The pronounced spikes, however, reveal volatility in the optimization process, suggesting that further smoothing or regularization could help stabilize training.","metadata":{}},{"cell_type":"code","source":"y_true_vgg, y_pred_vgg = get_y_true_and_y_pred(val_data, vgg_model, 0.5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:42:30.52966Z","iopub.execute_input":"2025-05-19T09:42:30.529955Z","iopub.status.idle":"2025-05-19T09:42:44.37725Z","shell.execute_reply.started":"2025-05-19T09:42:30.529934Z","shell.execute_reply":"2025-05-19T09:42:44.37646Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Confusion Matrix\ncm = confusion_matrix(y_true_vgg, y_pred_vgg)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisp.plot(cmap='Blues')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:42:44.378492Z","iopub.execute_input":"2025-05-19T09:42:44.378715Z","iopub.status.idle":"2025-05-19T09:42:44.533621Z","shell.execute_reply.started":"2025-05-19T09:42:44.378698Z","shell.execute_reply":"2025-05-19T09:42:44.532937Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Although we have lowered the false-negative rate, the remaining misclassifications still render this model inadequate for clinical application.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\n\nfpr, tpr, _ = roc_curve(y_true_vgg, y_pred_probs_vgg)\nroc_auc = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate (Recall)')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc='lower right')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T09:42:44.534321Z","iopub.execute_input":"2025-05-19T09:42:44.534569Z","iopub.status.idle":"2025-05-19T09:42:44.681121Z","shell.execute_reply.started":"2025-05-19T09:42:44.53454Z","shell.execute_reply":"2025-05-19T09:42:44.680358Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The model’s discriminative power has strengthened markedly, with AUC rising from 0.92 to 0.97—an excellent outcome. Next, I’ll evaluate the false-negative rate (1 – recall) to understand how often cancerous cases are missed.\n\nBelow is the (commented-out) submission script I used for the Kaggle leaderboard, which achieved a score of 0.7809.","metadata":{}},{"cell_type":"code","source":"#import os\n#import pandas as pd\n\n# Get all .tif filenames from the test folder\n#test_dir = \"/kaggle/input/histopathologic-cancer-detection/test/\"\n#test_filenames = sorted([f for f in os.listdir(test_dir) if f.endswith(\".tif\")])\n\n# Extract IDs by removing \".tif\"\n#test_ids = [f[:-4] for f in test_filenames]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T07:32:01.125022Z","iopub.execute_input":"2025-05-19T07:32:01.125284Z","iopub.status.idle":"2025-05-19T07:32:02.046175Z","shell.execute_reply.started":"2025-05-19T07:32:01.125264Z","shell.execute_reply":"2025-05-19T07:32:02.045436Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#import tensorflow as tf\n#import cv2\n#import numpy as np\n\n#def test_data_generator(paths):\n#    for path in paths:\n#        img = load_image_cv2(path)\n#        yield img\n\n# Create full paths\n#test_paths = [os.path.join(test_dir, fname) for fname in test_filenames]\n\n# Create tf.data.Dataset\n#test_dataset = tf.data.Dataset.from_generator(\n#    lambda: test_data_generator(test_paths),\n#    output_signature=tf.TensorSpec(shape=(96, 96, 3), dtype=tf.float32)\n#).batch(32).prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T07:32:03.41821Z","iopub.execute_input":"2025-05-19T07:32:03.418542Z","iopub.status.idle":"2025-05-19T07:32:03.515838Z","shell.execute_reply.started":"2025-05-19T07:32:03.418518Z","shell.execute_reply":"2025-05-19T07:32:03.515242Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#from tensorflow.keras.models import load_model\n\n#best_model = load_model('best_vgg_model_v3.keras')\n\n#pred_probs = best_model.predict(test_dataset, verbose=1)\n#pred_labels = (pred_probs > 0.5).astype(int).flatten()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T07:32:12.168575Z","iopub.execute_input":"2025-05-19T07:32:12.168845Z","execution_failed":"2025-05-19T07:34:26.619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#submission_df = pd.DataFrame({\n#    'id': test_ids,\n#    'label': pred_labels\n#})\n\n#submission_df.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-19T07:34:26.62Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, I’ll load the trained models and evaluate their performance by computing both accuracy and recall metrics.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import recall_score, accuracy_score\nfrom tensorflow.keras.models import load_model\n\nbasic_model = load_model('best_basic_model.keras')\ny_true_bm, y_pred_bm = get_y_true_and_y_pred(val_data, basic_model, 0.5)\n\nbm_rc = recall_score(y_true_bm, y_pred_bm)\nbm_ac = accuracy_score(y_true_bm, y_pred_bm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T05:27:20.449774Z","iopub.execute_input":"2025-05-20T05:27:20.450519Z","iopub.status.idle":"2025-05-20T05:28:58.57819Z","shell.execute_reply.started":"2025-05-20T05:27:20.450492Z","shell.execute_reply":"2025-05-20T05:28:58.577629Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import recall_score, accuracy_score\nfrom tensorflow.keras.models import load_model\n\nvgg_model = load_model('best_vgg_model.keras')\ny_true_vgg, y_pred_vgg = get_y_true_and_y_pred(val_data, vgg_model, 0.5)\n\nvgg_rc = recall_score(y_true_vgg, y_pred_vgg)\nvgg_ac = accuracy_score(y_true_vgg, y_pred_vgg)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T05:29:16.861136Z","iopub.execute_input":"2025-05-20T05:29:16.861421Z","iopub.status.idle":"2025-05-20T05:29:32.106784Z","shell.execute_reply.started":"2025-05-20T05:29:16.861397Z","shell.execute_reply":"2025-05-20T05:29:32.106198Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import recall_score, accuracy_score\nfrom tensorflow.keras.models import load_model\n\nvgg_model_lr = load_model('best_vgg_model_v1.keras')\ny_true_vgg_lr, y_pred_vgg_lr = get_y_true_and_y_pred(val_data, vgg_model_lr, 0.5)\n\nvgg_lr_rc = recall_score(y_true_vgg_lr, y_pred_vgg_lr)\nvgg_lr_ac = accuracy_score(y_true_vgg_lr, y_pred_vgg_lr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T05:32:25.669943Z","iopub.execute_input":"2025-05-20T05:32:25.670821Z","iopub.status.idle":"2025-05-20T05:32:39.65338Z","shell.execute_reply.started":"2025-05-20T05:32:25.670795Z","shell.execute_reply":"2025-05-20T05:32:39.652541Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import recall_score, accuracy_score\nfrom tensorflow.keras.models import load_model\n\nvgg_model_v2 = load_model('best_vgg_model_v2.keras')\ny_true_vgg_v2, y_pred_vgg_v2 = get_y_true_and_y_pred(val_data, vgg_model_v2, 0.5)\n\nvgg_v2_rc = recall_score(y_true_vgg_v2, y_pred_vgg_v2)\nvgg_v2_ac = accuracy_score(y_true_vgg_v2, y_pred_vgg_v2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T05:32:39.654748Z","iopub.execute_input":"2025-05-20T05:32:39.655205Z","iopub.status.idle":"2025-05-20T05:32:53.858371Z","shell.execute_reply.started":"2025-05-20T05:32:39.655185Z","shell.execute_reply":"2025-05-20T05:32:53.857834Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import recall_score, accuracy_score\nfrom tensorflow.keras.models import load_model\n\nvgg_model_v3 = load_model('best_vgg_model_v3.keras')\ny_true_vgg_v3, y_pred_vgg_v3 = get_y_true_and_y_pred(val_data, vgg_model_v3, 0.5)\n\nvgg_v3_rc = recall_score(y_true_vgg_v3, y_pred_vgg_v3)\nvgg_v3_ac = accuracy_score(y_true_vgg_v3, y_pred_vgg_v3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T05:32:55.963708Z","iopub.execute_input":"2025-05-20T05:32:55.964346Z","iopub.status.idle":"2025-05-20T05:33:09.963045Z","shell.execute_reply.started":"2025-05-20T05:32:55.964323Z","shell.execute_reply":"2025-05-20T05:33:09.96245Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"My primary concern is to minimize the false-negative rate—misclassifying a cancer-positive patient as healthy is simply unacceptable in a medical context.","metadata":{}},{"cell_type":"code","source":"results = {\n    'Model': ['Basic Model', 'VGG Model', 'VGG Model with 1e-4 LR', 'VGG Model v2', 'VGG Model v3'],\n    'Accuracy': [bm_ac, vgg_ac, vgg_lr_ac, vgg_v2_ac, vgg_v3_ac],\n    'Recall': [bm_rc, vgg_rc, vgg_lr_rc, vgg_v2_rc, vgg_v3_rc],\n    'Miss Rate': [1 - bm_rc, 1 - vgg_rc, 1 - vgg_lr_rc, 1 - vgg_v2_rc, 1 - vgg_v3_rc]\n}\n\ndf_result = pd.DataFrame(results).sort_values(by=['Miss Rate'], ascending=True)\ndf_result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T05:44:59.878407Z","iopub.execute_input":"2025-05-20T05:44:59.87913Z","iopub.status.idle":"2025-05-20T05:44:59.889787Z","shell.execute_reply.started":"2025-05-20T05:44:59.879103Z","shell.execute_reply":"2025-05-20T05:44:59.889041Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I observed consistent performance gains with each tuning iteration, which suggests that the original model suffered from substantial overfitting. The introduction of dropout and L2 penalties clearly mitigated this issue, but additional refinements are warranted:\n\n- Expand the dataset: Increasing sample size would help the model generalize more robustly.\n\n- Experiment with optimizers: Trying alternatives such as SGD with momentum or AdamW could further enhance convergence.\n\n- Deepen the architecture: Adding more VGG blocks or exploring hybrid variants may capture richer feature hierarchies.\n\n- Evaluate other architectures: Given the early-stopping behavior, testing InceptionNet or ResNet could yield higher stability.\n\n- Extend training duration: Running for additional epochs with careful regularization might squeeze out further gains.\n\n## Conclusion\n\nWhile this configuration achieved a respectable Kaggle test score of 0.7809, I would be hesitant to deploy it in a medical setting without additional validation. In my view, the project has a strong foundation, yet substantial work remains—particularly around false-negative reduction and architectural exploration—to reach a level of reliability appropriate for clinical use.","metadata":{}},{"cell_type":"markdown","source":"# References\n\nAitken, A. M. (2025). Titanic - ML from Disaster (Supervised Learning), 8. Retrieved 05/13/2025 from https://www.kaggle.com/code/alexandermaitken/titanic-ml-from-disaster-supervised-learning#Titanic---ML-from-Disaster\n\nCukierski, W. (2018). Histopathologic Cancer Detection, 1. Retrieved 04/10/2025 from https://www.kaggle.com/competitions/histopathologic-cancer-detection.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}