{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-30T00:08:36.144137Z","iopub.execute_input":"2025-05-30T00:08:36.144704Z","iopub.status.idle":"2025-05-30T00:08:36.149180Z","shell.execute_reply.started":"2025-05-30T00:08:36.144680Z","shell.execute_reply":"2025-05-30T00:08:36.148232Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Disaster Tweets Classification with LSTM + GAN Augmentation\n\nThis notebook combines:\n\n1. **Deep-learning EDA** to choose sensible parameters.  \n2. A **Bi-LSTM classifier** (your “LTMS” model).  \n3. A simple **conditional GAN** to augment the training set.  \n4. Retraining the LSTM on the augmented data for improved performance.  ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import Counter\n\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import Input, Model\nfrom tensorflow.keras.layers import (\n    Embedding, Bidirectional, LSTM, Dense, Dropout, \n    Concatenate, Reshape, Flatten\n)\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T00:08:36.160199Z","iopub.execute_input":"2025-05-30T00:08:36.160871Z","iopub.status.idle":"2025-05-30T00:08:36.178236Z","shell.execute_reply.started":"2025-05-30T00:08:36.160844Z","shell.execute_reply":"2025-05-30T00:08:36.177332Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load Kaggle datasets\ntrain_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest_df  = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\nsub_df   = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T00:08:36.179935Z","iopub.execute_input":"2025-05-30T00:08:36.180309Z","iopub.status.idle":"2025-05-30T00:08:36.234700Z","shell.execute_reply.started":"2025-05-30T00:08:36.180279Z","shell.execute_reply":"2025-05-30T00:08:36.233862Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 2: Text Cleaning & Field-Combining  \n\nWe lowercase, strip URLs/mentions, remove non-alphanumerics, then **concatenate** text + keyword + location to give the model every signal available.\n","metadata":{}},{"cell_type":"code","source":"def clean_text(s):\n    if pd.isna(s): return \"\"\n    s = s.lower()\n    s = re.sub(r'http\\S+', '', s)\n    s = re.sub(r'@\\w+', '', s)\n    s = re.sub(r'[^a-z0-9\\s]', '', s)\n    return re.sub(r'\\s+', ' ', s).strip()\n\nfor df in (train_df, test_df):\n    df['clean_text']  = df['text'].apply(clean_text)\n    df['clean_kw']    = df['keyword'].apply(clean_text)\n    df['clean_loc']   = df['location'].apply(clean_text)\n    df['model_input'] = (\n        df['clean_text'] + ' ' +\n        df['clean_kw'].replace('', '') + ' ' +\n        df['clean_loc'].replace('', '')\n    ).str.strip()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T00:08:36.235591Z","iopub.execute_input":"2025-05-30T00:08:36.235898Z","iopub.status.idle":"2025-05-30T00:08:36.451683Z","shell.execute_reply.started":"2025-05-30T00:08:36.235872Z","shell.execute_reply":"2025-05-30T00:08:36.450419Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 3: Class Balance  \n**What you’ll see:** A bar chart of non-disaster vs disaster counts.  \nHelps decide if we need class weighting or oversampling.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(5,4))\ntrain_df['target'].value_counts().plot(kind='bar', color=['#777','#c44'])\nplt.xticks([0,1], ['Not Disaster (0)','Disaster (1)'], rotation=0)\nplt.ylabel('Count'); plt.title('Target Class Distribution')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T00:08:36.452690Z","iopub.execute_input":"2025-05-30T00:08:36.452972Z","iopub.status.idle":"2025-05-30T00:08:36.589893Z","shell.execute_reply.started":"2025-05-30T00:08:36.452950Z","shell.execute_reply":"2025-05-30T00:08:36.589054Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 4: Character Length Distribution  \n**What you’ll see:** Histogram of tweet lengths in characters.  \nGuides choice of `maxlen` for padding/truncation.","metadata":{}},{"cell_type":"code","source":"train_df['char_len'] = train_df['model_input'].str.len()\nplt.figure(figsize=(6,4))\nplt.hist(train_df['char_len'], bins=30, edgecolor='k')\nplt.xlabel('Chars'); plt.ylabel('Tweets'); plt.title('Tweet Length (chars)')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T00:08:36.592475Z","iopub.execute_input":"2025-05-30T00:08:36.592786Z","iopub.status.idle":"2025-05-30T00:08:37.319668Z","shell.execute_reply.started":"2025-05-30T00:08:36.592760Z","shell.execute_reply":"2025-05-30T00:08:37.318816Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 5: Token Count Distribution  \n**What you’ll see:** Histogram of word-counts per tweet.  \nHelps set sequence length for the Embedding + LSTM.","metadata":{}},{"cell_type":"code","source":"train_df['token_count'] = train_df['model_input'].str.split().apply(len)\nplt.figure(figsize=(6,4))\nplt.hist(train_df['token_count'], bins=30, edgecolor='k')\nplt.xlabel('Tokens'); plt.ylabel('Tweets'); plt.title('Tokens per Tweet')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T00:08:37.320627Z","iopub.execute_input":"2025-05-30T00:08:37.321425Z","iopub.status.idle":"2025-05-30T00:08:37.552471Z","shell.execute_reply.started":"2025-05-30T00:08:37.321397Z","shell.execute_reply":"2025-05-30T00:08:37.551626Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 6: Vocabulary & Rare Words  \n**What you’ll see:**  \n- **Total vocab size**  \n- **% of tokens** that appear ≤5× (“long tail”)  \nInforms your `num_words` cutoff in the Tokenizer.","metadata":{}},{"cell_type":"code","source":"wc = Counter()\nfor txt in train_df['model_input']:\n    wc.update(txt.split())\n\nvocab_size = len(wc)\nrare_words = sum(1 for _,c in wc.items() if c <= 5)\nprint(f\"Vocab size: {vocab_size}\")\nprint(f\"Rare (≤5×): {rare_words} ({rare_words/vocab_size*100:.1f}%)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T00:08:37.553394Z","iopub.execute_input":"2025-05-30T00:08:37.553709Z","iopub.status.idle":"2025-05-30T00:08:37.592404Z","shell.execute_reply.started":"2025-05-30T00:08:37.553684Z","shell.execute_reply":"2025-05-30T00:08:37.591287Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cell 7: Word-Frequency Distribution  \n**What you’ll see:** Log-scale histogram of token frequencies.  \nReveals Zipf’s law: most words are rare.","metadata":{}},{"cell_type":"code","source":"freqs = np.array(list(wc.values()))\nplt.figure(figsize=(6,4))\nplt.hist(freqs, bins=50, log=True, edgecolor='k')\nplt.xlabel('Freq'); plt.ylabel('Tokens'); plt.title('Word Frequency Dist (log y)')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T00:08:37.593399Z","iopub.execute_input":"2025-05-30T00:08:37.594048Z","iopub.status.idle":"2025-05-30T00:08:38.089357Z","shell.execute_reply.started":"2025-05-30T00:08:37.594015Z","shell.execute_reply":"2025-05-30T00:08:38.088496Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 8: Tokenization & Sequence Prep  \n- **num_words** = 20 000  \n- **maxlen**   = 50  ","metadata":{}},{"cell_type":"code","source":"MAX_VOCAB = 20000\nMAX_LEN   = 50\n\ntokenizer = Tokenizer(num_words=MAX_VOCAB, oov_token='<OOV>')\ntokenizer.fit_on_texts(train_df['model_input'])\n\ndef to_seq(texts):\n    seq = tokenizer.texts_to_sequences(texts)\n    return pad_sequences(seq, maxlen=MAX_LEN, padding='post', truncating='post')\n\nX = to_seq(train_df['model_input'])\ny = train_df['target'].values\nX_test = to_seq(test_df['model_input'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T00:08:38.090214Z","iopub.execute_input":"2025-05-30T00:08:38.090464Z","iopub.status.idle":"2025-05-30T00:08:38.413826Z","shell.execute_reply.started":"2025-05-30T00:08:38.090445Z","shell.execute_reply":"2025-05-30T00:08:38.413066Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 9: Build Bi-LSTM Classifier  \nThis is your “LTMS” model. ","metadata":{}},{"cell_type":"code","source":"emb_dim = 64\ninp = Input(shape=(MAX_LEN,))\nx   = Embedding(MAX_VOCAB, emb_dim, input_length=MAX_LEN)(inp)\nx   = Bidirectional(LSTM(64))(x)\nx   = Dropout(0.5)(x)\nx   = Dense(32, activation='relu')(x)\nx   = Dropout(0.5)(x)\nout = Dense(1, activation='sigmoid')(x)\nclassifier = Model(inp, out)\nclassifier.compile(\n    loss='binary_crossentropy',\n    optimizer=Adam(1e-3),\n    metrics=['accuracy']\n)\nclassifier.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T00:08:38.414710Z","iopub.execute_input":"2025-05-30T00:08:38.415040Z","iopub.status.idle":"2025-05-30T00:08:38.517092Z","shell.execute_reply.started":"2025-05-30T00:08:38.415013Z","shell.execute_reply":"2025-05-30T00:08:38.516187Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 10: Baseline Training  ","metadata":{}},{"cell_type":"code","source":"X_tr, X_val, y_tr, y_val = train_test_split(\n    X, y, test_size=0.1, stratify=y, random_state=42\n)\nes = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\nmc = ModelCheckpoint('best_lstm.h5', save_best_only=True)\n\nhistory = classifier.fit(\n    X_tr, y_tr,\n    validation_data=(X_val, y_val),\n    epochs=10, batch_size=128,\n    callbacks=[es,mc]\n)\nprint(\"Val accuracy:\", classifier.evaluate(X_val, y_val, verbose=0)[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T00:08:38.518077Z","iopub.execute_input":"2025-05-30T00:08:38.518367Z","iopub.status.idle":"2025-05-30T00:09:10.456108Z","shell.execute_reply.started":"2025-05-30T00:08:38.518347Z","shell.execute_reply":"2025-05-30T00:09:10.455313Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 11: Conditional GAN for Embedding-Space Augmentation  \n\nWe train a simple GAN that **generates synthetic embedding sequences** conditioned on class label.  \nLater we sample from the generator to augment the LSTM’s training set.","metadata":{}},{"cell_type":"code","source":"# 11a: Generator\nnoise_dim = 100\nlabel_in  = Input(shape=(1,), dtype='int32')\nnoise_in  = Input(shape=(noise_dim,))\n\n# embed label and concat\nlbl_emb = Embedding(2, noise_dim, input_length=1)(label_in)\nlbl_emb = Flatten()(lbl_emb)\n\ng_in = Concatenate()([noise_in, lbl_emb])\ng   = Dense(128, activation='relu')(g_in)\ng   = Dense(MAX_LEN * emb_dim, activation='tanh')(g)\ng   = Reshape((MAX_LEN, emb_dim))(g)\ngenerator = Model([noise_in, label_in], g)\ngenerator.summary()\n\n# 11b: Discriminator\nseq_in = Input(shape=(MAX_LEN, emb_dim))\nd_lbl  = Input(shape=(1,), dtype='int32')\nx      = LSTM(64)(seq_in)\nx      = Concatenate()([x, Flatten()(Embedding(2, emb_dim, input_length=1)(d_lbl))])\nx      = Dense(64, activation='relu')(x)\n\n# Real/Fake output + Aux classifier for label\nvalidity = Dense(1, activation='sigmoid', name='real_fake')(x)\naux_cls  = Dense(2, activation='softmax', name='aux_out')(x)\ndiscriminator = Model([seq_in, d_lbl], [validity, aux_cls])\ndiscriminator.compile(\n    loss=['binary_crossentropy','sparse_categorical_crossentropy'],\n    optimizer=Adam(2e-4),\n    metrics=['accuracy']\n)\ndiscriminator.summary()\n\n# 11c: Combined GAN\ndiscriminator.trainable = False\ngen_seq, gen_lbl = generator([noise_in, label_in]), label_in\nvalid, pred_lbl   = discriminator([gen_seq, gen_lbl])\ngan = Model([noise_in, label_in], [valid, pred_lbl])\ngan.compile(\n    loss=['binary_crossentropy','sparse_categorical_crossentropy'],\n    optimizer=Adam(2e-4)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T00:09:10.457427Z","iopub.execute_input":"2025-05-30T00:09:10.457660Z","iopub.status.idle":"2025-05-30T00:09:10.585193Z","shell.execute_reply.started":"2025-05-30T00:09:10.457642Z","shell.execute_reply":"2025-05-30T00:09:10.584306Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 12: Train GAN  ","metadata":{}},{"cell_type":"code","source":"# --- after you’ve built `generator` and `discriminator` ---\n\nfrom tensorflow.keras.optimizers import Adam\n\n# --- 1) Build generator & discriminator as before ---\n\n# --- 2) Compile discriminator FIRST, with trainable=True ---\ndiscriminator.trainable = True\ndiscriminator.compile(\n    optimizer=Adam(2e-4),\n    loss=['binary_crossentropy','sparse_categorical_crossentropy'],\n    metrics=['accuracy','accuracy']   # one accuracy per output\n)\n\n# --- 3) Now freeze discriminator, build & compile the GAN ---\ndiscriminator.trainable = False\ngenerator.trainable     = True\n\nnoise_in = Input(shape=(noise_dim,))\nlabel_in = Input(shape=(1,), dtype='int32')\n\ngen_seq, gen_lbl = generator([noise_in, label_in]), label_in\nvalid, pred_lbl  = discriminator([gen_seq, gen_lbl])\ngan = Model([noise_in, label_in], [valid, pred_lbl])\ngan.compile(\n    optimizer=Adam(2e-4),\n    loss=['binary_crossentropy','sparse_categorical_crossentropy']\n)\n\n# --- 4) Training loop: explicitly toggle trainable flags before each step ---\nbatch_size = 64\nsteps      = 1000\n\n# precompute embeddings once\nembed_layer = classifier.layers[1]\nembeddings  = embed_layer(tf.constant(X)).numpy()\nlabels      = y\n\nfor step in range(steps):\n    # (a) Sample real\n    idx       = np.random.randint(0, embeddings.shape[0], batch_size)\n    real_seqs = embeddings[idx]\n    real_lbls = labels[idx].reshape(-1,1)\n\n    # (b) Generate fake\n    noise     = np.random.normal(size=(batch_size, noise_dim))\n    fake_lbls = np.random.randint(0,2,size=(batch_size,1))\n    fake_seqs = generator.predict([noise, fake_lbls], verbose=0)\n\n    # (c) Train discriminator\n    discriminator.trainable = True   # ensure weights are unfrozen\n    d_loss_real = discriminator.train_on_batch(\n        [real_seqs, real_lbls],\n        [np.ones((batch_size,1)), real_lbls]\n    )\n    d_loss_fake = discriminator.train_on_batch(\n        [fake_seqs, fake_lbls],\n        [np.zeros((batch_size,1)), fake_lbls]\n    )\n\n    # (d) Train generator via the GAN model\n    discriminator.trainable = False  # freeze discriminator for GAN pass\n    g_loss = gan.train_on_batch(\n        [noise, fake_lbls],\n        [np.ones((batch_size,1)), fake_lbls]\n    )\n\n    if step % 200 == 0:\n        print(f\"{step:4d}  d_real={d_loss_real[0]:.3f}, d_fake={d_loss_fake[0]:.3f}, g={g_loss[0]:.3f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T00:09:10.586027Z","iopub.execute_input":"2025-05-30T00:09:10.586270Z","iopub.status.idle":"2025-05-30T00:12:09.019987Z","shell.execute_reply.started":"2025-05-30T00:09:10.586250Z","shell.execute_reply":"2025-05-30T00:12:09.019080Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 13: Augment & Retrain LSTM  \nGenerate **N** synthetic sequences per class, append to training data, then retrain.","metadata":{}},{"cell_type":"code","source":"# %% Cell: Synthetic Sequence Generation & LSTM Retraining\n\n# 0) Imports (with fallback)\ntry:\n    from tensorflow.keras.layers   import Input, LSTM, Dropout, Dense\n    from tensorflow.keras.models   import Model\n    from tensorflow.keras.optimizers import Adam\n    from tensorflow.keras.callbacks  import EarlyStopping, ModelCheckpoint\nexcept ImportError:\n    from keras.layers   import Input, LSTM, Dropout, Dense\n    from keras.models   import Model\n    from keras.optimizers import Adam\n    from keras.callbacks  import EarlyStopping, ModelCheckpoint\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# 1) Assumed pre-existing variables in your notebook:\n#    - generator   : your trained GAN generator\n#    - embeddings  : NumPy array, shape = (num_samples, MAX_LEN, emb_dim)\n#    - labels      : NumPy array, shape = (num_samples,) of 0/1\n#    - noise_dim   : int, noise vector size\n#    - MAX_LEN     : int, sequence length\n#    - emb_dim     : int, embedding dimension\n\n# 2) Generate synthetic embeddings\nN     = 2000\nnoise = np.random.normal(size=(2*N, noise_dim)).astype('float32')\nlbls  = np.array([0]*N + [1]*N, dtype='int32').reshape(-1,1)\n\nsyn_seqs = generator.predict([noise, lbls], verbose=0)  # → shape (2N, MAX_LEN, emb_dim)\n\n# 3) Build augmented dataset\nX_aug = np.vstack([embeddings, syn_seqs])\ny_aug = np.concatenate([labels, lbls.ravel()])\n\n# 4) Define a new LSTM classifier operating on embeddings\ninp2 = Input(shape=(MAX_LEN, emb_dim))\nx2   = LSTM(64)(inp2)\nx2   = Dropout(0.5)(x2)\nx2   = Dense(32, activation='relu')(x2)\nout2 = Dense(1, activation='sigmoid')(x2)\n\nclf2 = Model(inp2, out2)\nclf2.compile(\n    loss='binary_crossentropy',\n    optimizer=Adam(1e-3),\n    metrics=['accuracy']\n)\n\n# 5) Callbacks\nes2 = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\nmc2 = ModelCheckpoint('best_aug_lstm.h5', save_best_only=True)\n\n# 6) Train/test split\nXa_tr, Xa_val, ya_tr, ya_val = train_test_split(\n    X_aug, y_aug,\n    test_size=0.1,\n    stratify=y_aug,\n    random_state=42\n)\n\n# 7) Retrain on augmented data\nhistory2 = clf2.fit(\n    Xa_tr, ya_tr,\n    validation_data=(Xa_val, ya_val),\n    epochs=10,\n    batch_size=128,\n    callbacks=[es2, mc2]\n)\n\n# 8) Evaluate\nval_loss, val_acc = clf2.evaluate(Xa_val, ya_val, verbose=0)\nprint(f\"Augmented LSTM validation accuracy: {val_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T00:15:41.133475Z","iopub.execute_input":"2025-05-30T00:15:41.134125Z","iopub.status.idle":"2025-05-30T00:16:01.107120Z","shell.execute_reply.started":"2025-05-30T00:15:41.134098Z","shell.execute_reply":"2025-05-30T00:16:01.106220Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cell 14: Final Prediction & Submission  \nUse the **embeddings + augmented classifier** to predict on test set.","metadata":{}},{"cell_type":"code","source":"# embed test inputs\ntest_emb = embed_layer(tf.constant(X_test)).numpy()\npreds    = (clf2.predict(test_emb) > 0.5).astype(int).ravel()\n\nsub_df['target'] = preds\nsub_df.to_csv('submission.csv', index=False)\nprint(\"Saved submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T00:16:59.475770Z","iopub.execute_input":"2025-05-30T00:16:59.476137Z","iopub.status.idle":"2025-05-30T00:17:01.048904Z","shell.execute_reply.started":"2025-05-30T00:16:59.476113Z","shell.execute_reply":"2025-05-30T00:17:01.048069Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Conclusion\n\nIn this notebook we carried out a deep‐learning driven pipeline for disaster‐tweet classification. Our **EDA** revealed a moderate class imbalance (≈60/40), typical tweet lengths under 50 tokens, and a Zipfian vocabulary with a long tail—guiding our choices of `maxlen=50`, `num_words=20 000`, and potential class-weighting.  \n\nWe then built a **Bi-LSTM (‘LTMS’) classifier** as a strong baseline, achieving solid validation accuracy while leveraging both the tweet text and auxiliary fields (keyword, location). To further improve, we implemented a **conditional GAN in embedding space** to synthesize additional labeled examples, addressing data scarcity for rare examples.  \n\nAugmenting our real data with GAN-generated embeddings and retraining the LSTM yielded measurable gains in validation accuracy, demonstrating the benefit of targeted synthetic augmentation.  \n\n**Next steps** could include:  \n- Swapping the Bi-LSTM for a lightweight Transformer (e.g. DistilBERT) with fine-tuning.  \n- Incorporating pretrained embeddings (GloVe, FastText, or domain-specific Twitter embeddings).  \n- Exploring more advanced sequence-GAN architectures (e.g. SeqGAN or WGAN) for richer text generation.  \n- Performing thorough hyperparameter sweeps and k-fold cross-validation to solidify your model’s generalization.  \n\nOverall, this workflow—combining targeted EDA, a robust LSTM backbone, and GAN-driven augmentation—provides a strong, extensible foundation for tackling text classification in low-resource or imbalanced settings.  \n\n\n## GitHub Respository\nhttps://github.com/DJBlom/cu-boulder-ms-cs/tree/main/machine-learning/introduction-to-deep-learning/competition\n","metadata":{}}]}